{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SudhakarReddy1602/Neural_Assignments/blob/main/700756157_icp6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sudhakar Reddy Venna\n",
        "700756157"
      ],
      "metadata": {
        "id": "qV2DlMvTkm4D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "from matplotlib import pyplot\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import to_categorical\n",
        "import re\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "YDXwKmU0bL7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csUKnEP4aTs2",
        "outputId": "1f96a628-b4c8-4801-ce67-6597dd885277"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "291/291 - 44s - loss: 0.8249 - accuracy: 0.6451 - 44s/epoch - 152ms/step\n",
            "144/144 - 2s - loss: 0.7482 - accuracy: 0.6640 - 2s/epoch - 16ms/step\n",
            "0.7481690049171448\n",
            "0.6640454530715942\n",
            "['loss', 'accuracy']\n"
          ]
        }
      ],
      "source": [
        "data = pd.read_csv('/content/Sentiment.csv')\n",
        "# Keeping only the neccessary columns\n",
        "data = data[['text','sentiment']]\n",
        "\n",
        "data['text'] = data['text'].apply(lambda x: x.lower())\n",
        "data['text'] = data['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]', '', x)))\n",
        "\n",
        "for idx, row in data.iterrows():\n",
        "    row[0] = row[0].replace('rt', ' ')\n",
        "\n",
        "max_features = 2000\n",
        "tokenizer = Tokenizer(num_words=max_features, split=' ')\n",
        "tokenizer.fit_on_texts(data['text'].values)\n",
        "X = tokenizer.texts_to_sequences(data['text'].values)\n",
        "\n",
        "X = pad_sequences(X)\n",
        "\n",
        "embed_dim = 128\n",
        "lstm_out = 196\n",
        "def createmodel():\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(max_features, embed_dim,input_length = X.shape[1]))\n",
        "    model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
        "    model.add(Dense(3,activation='softmax'))\n",
        "    model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
        "    return model\n",
        "# print(model.summary())\n",
        "\n",
        "labelencoder = LabelEncoder()\n",
        "integer_encoded = labelencoder.fit_transform(data['sentiment'])\n",
        "y = to_categorical(integer_encoded)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X,y, test_size = 0.33, random_state = 42)\n",
        "\n",
        "batch_size = 32\n",
        "model = createmodel()\n",
        "model.fit(X_train, Y_train, epochs = 1, batch_size=batch_size, verbose = 2)\n",
        "score,acc = model.evaluate(X_test,Y_test,verbose=2,batch_size=batch_size)\n",
        "print(score)\n",
        "print(acc)\n",
        "print(model.metrics_names)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('SentimentAnalysis.h5')"
      ],
      "metadata": {
        "id": "kYWzz9tTc11j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a4e4111-2b64-48bc-dbdd-587fdb859405"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tweepy\n",
        "from keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import re"
      ],
      "metadata": {
        "id": "07-JzJpQf1Re"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved model\n",
        "model = load_model('/content/SentimentAnalysis.h5')\n",
        "\n",
        "# Define a function for preprocessing text\n",
        "def preprocess_data(text):\n",
        "  text = text.lower()\n",
        "  text = re.sub('[^a-zA-z0-9\\s]', '', text)\n",
        "  return text\n",
        "\n",
        "new_data = \"A lot of good things are happening. We are respected again throughout the world, and that's a great thing\"\n",
        "# Preprocess the new text data\n",
        "new_data = preprocess_data(new_data)\n",
        "\n",
        "# Tokenize and pad the new text data\n",
        "max_features = 2000\n",
        "tokenizer = Tokenizer(num_words=max_features, split=' ')\n",
        "tokenizer.fit_on_texts([new_data])\n",
        "X_new = tokenizer.texts_to_sequences([new_data])\n",
        "X_new = pad_sequences(X_new, maxlen=model.input_shape[1])\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(X_new)\n",
        "\n",
        "# Determine the sentiment based on the prediction\n",
        "sentiments = ['Negative', 'Neutral', 'Positive']\n",
        "predicted_sentiment = sentiments[predictions.argmax()]\n",
        "\n",
        "# Print the result\n",
        "print(\"Predicted Sentiment: \" + predicted_sentiment)"
      ],
      "metadata": {
        "id": "4rSCUiEndrv3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e7a50f4-cfd8-4319-d599-a7a10365bfca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 398ms/step\n",
            "Predicted Sentiment: Negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply GridSearchCV on the source code"
      ],
      "metadata": {
        "id": "TXMVAAn3CfVB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikeras\n",
        "from scikeras.wrappers import KerasClassifier"
      ],
      "metadata": {
        "id": "h57eQCf_CqkY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e9a01674-c4ac-4dd0-fa66-6d111353c1af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikeras\n",
            "  Downloading scikeras-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting keras>=3.2.0 (from scikeras)\n",
            "  Downloading keras-3.4.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scikit-learn>=1.4.2 (from scikeras)\n",
            "  Downloading scikit_learn-1.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (1.25.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (13.7.1)\n",
            "Collecting namex (from keras>=3.2.0->scikeras)\n",
            "  Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (3.9.0)\n",
            "Collecting optree (from keras>=3.2.0->scikeras)\n",
            "  Downloading optree-0.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (347 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.7/347.7 kB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (0.2.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (24.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.4.2->scikeras) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras>=3.2.0->scikeras) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->scikeras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->scikeras) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->scikeras) (0.1.2)\n",
            "Installing collected packages: namex, optree, scikit-learn, keras, scikeras\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.15.0\n",
            "    Uninstalling keras-2.15.0:\n",
            "      Successfully uninstalled keras-2.15.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.4.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-3.4.1 namex-0.0.8 optree-0.12.1 scikeras-0.13.0 scikit-learn-1.5.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "keras"
                ]
              },
              "id": "3687de895e59400094457793ce618f2a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name '_fit_context' from 'sklearn.base' (/usr/local/lib/python3.10/dist-packages/sklearn/base.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-937e0ebd9588>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install scikeras'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscikeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrappers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKerasClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scikeras/wrappers.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscikeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mloss_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscikeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_random_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscikeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClassifierLabelEncoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRegressorTargetEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scikeras/utils/transformers.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseEstimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTransformerMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmake_pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFunctionTransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOrdinalEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulticlass\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTransformerMixin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_fit_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFunctionTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name '_fit_context' from 'sklearn.base' (/usr/local/lib/python3.10/dist-packages/sklearn/base.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "\n",
        "# Assuming the data loading and preprocessing steps are the same\n",
        "model = load_model('/content/SentimentAnalysis.h5')\n",
        "max_features = 2000\n",
        "tokenizer = Tokenizer(num_words=max_features, split=' ')\n",
        "# Assuming tokenizer fitting and text preprocessing is done here\n",
        "\n",
        "def createmodel(optimizer='adam'):\n",
        "    model1 = Sequential()\n",
        "    model1.add(Embedding(max_features, embed_dim, input_length=X.shape[1]))\n",
        "    model1.add(SpatialDropout1D(0.2))\n",
        "    model1.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
        "    model1.add(Dense(3, activation='softmax'))\n",
        "    model1.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Define the KerasClassifier with the build_fn as our model creation function\n",
        "model = KerasClassifier(model, verbose=2)\n",
        "\n",
        "# Define hyperparameters to tune\n",
        "param_grid = {\n",
        "    'batch_size': [32, 64],\n",
        "    'epochs': [1, 2],\n",
        "    'optimizer': ['adam', 'rmsprop']\n",
        "}"
      ],
      "metadata": {
        "id": "nG_0dFChDf0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize GridSearchCV\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1, cv=3)\n"
      ],
      "metadata": {
        "id": "vm5T8RxvDwXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit GridSearchCV\n",
        "grid_result = grid.fit(X_train, Y_train)\n",
        "\n",
        "# Summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3El5fSP3Le2x",
        "outputId": "cd650afc-f7f4-429a-9023-576d02150311"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_lib.py:576: UserWarning: Skipping variable loading for optimizer 'adam', because it has 14 variables whereas the saved optimizer has 2 variables. \n",
            "  saveable.load_own_variables(weights_store.get(inner_path))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "194/194 - 22s - 114ms/step - accuracy: 0.7104 - loss: 0.6909\n",
            "97/97 - 1s - 15ms/step\n",
            "194/194 - 11s - 59ms/step - accuracy: 0.7123 - loss: 0.6886\n",
            "97/97 - 1s - 15ms/step\n",
            "194/194 - 11s - 58ms/step - accuracy: 0.7063 - loss: 0.6792\n",
            "97/97 - 1s - 15ms/step\n",
            "194/194 - 11s - 59ms/step - accuracy: 0.7104 - loss: 0.6883\n",
            "97/97 - 2s - 20ms/step\n",
            "194/194 - 11s - 58ms/step - accuracy: 0.7062 - loss: 0.6817\n",
            "97/97 - 2s - 16ms/step\n",
            "194/194 - 11s - 55ms/step - accuracy: 0.7103 - loss: 0.6759\n",
            "97/97 - 3s - 31ms/step\n",
            "Epoch 1/2\n",
            "194/194 - 10s - 52ms/step - accuracy: 0.7103 - loss: 0.6913\n",
            "Epoch 2/2\n",
            "194/194 - 10s - 54ms/step - accuracy: 0.7393 - loss: 0.6235\n",
            "97/97 - 1s - 15ms/step\n",
            "Epoch 1/2\n",
            "194/194 - 11s - 56ms/step - accuracy: 0.7119 - loss: 0.6802\n",
            "Epoch 2/2\n",
            "194/194 - 10s - 50ms/step - accuracy: 0.7411 - loss: 0.6163\n",
            "97/97 - 1s - 15ms/step\n",
            "Epoch 1/2\n",
            "194/194 - 11s - 58ms/step - accuracy: 0.7119 - loss: 0.6725\n",
            "Epoch 2/2\n",
            "194/194 - 12s - 61ms/step - accuracy: 0.7489 - loss: 0.5998\n",
            "97/97 - 2s - 21ms/step\n",
            "Epoch 1/2\n",
            "194/194 - 11s - 58ms/step - accuracy: 0.7099 - loss: 0.6918\n",
            "Epoch 2/2\n",
            "194/194 - 10s - 51ms/step - accuracy: 0.7366 - loss: 0.6241\n",
            "97/97 - 1s - 15ms/step\n",
            "Epoch 1/2\n",
            "194/194 - 11s - 58ms/step - accuracy: 0.7128 - loss: 0.6777\n",
            "Epoch 2/2\n",
            "194/194 - 10s - 53ms/step - accuracy: 0.7445 - loss: 0.6111\n",
            "97/97 - 1s - 15ms/step\n",
            "Epoch 1/2\n",
            "194/194 - 11s - 58ms/step - accuracy: 0.7121 - loss: 0.6793\n",
            "Epoch 2/2\n",
            "194/194 - 10s - 49ms/step - accuracy: 0.7402 - loss: 0.6070\n",
            "97/97 - 2s - 18ms/step\n",
            "97/97 - 13s - 133ms/step - accuracy: 0.7091 - loss: 0.6846\n",
            "49/49 - 1s - 18ms/step\n",
            "97/97 - 6s - 67ms/step - accuracy: 0.7133 - loss: 0.6747\n",
            "49/49 - 1s - 29ms/step\n",
            "97/97 - 6s - 61ms/step - accuracy: 0.7116 - loss: 0.6697\n",
            "49/49 - 1s - 18ms/step\n",
            "97/97 - 7s - 74ms/step - accuracy: 0.7104 - loss: 0.6844\n",
            "49/49 - 1s - 18ms/step\n",
            "97/97 - 7s - 73ms/step - accuracy: 0.7136 - loss: 0.6749\n",
            "49/49 - 1s - 18ms/step\n",
            "97/97 - 7s - 72ms/step - accuracy: 0.7161 - loss: 0.6635\n",
            "49/49 - 1s - 18ms/step\n",
            "Epoch 1/2\n",
            "97/97 - 8s - 80ms/step - accuracy: 0.7104 - loss: 0.6814\n",
            "Epoch 2/2\n",
            "97/97 - 9s - 91ms/step - accuracy: 0.7393 - loss: 0.6192\n",
            "49/49 - 1s - 18ms/step\n",
            "Epoch 1/2\n",
            "97/97 - 6s - 67ms/step - accuracy: 0.7130 - loss: 0.6779\n",
            "Epoch 2/2\n",
            "97/97 - 6s - 60ms/step - accuracy: 0.7432 - loss: 0.6093\n",
            "49/49 - 2s - 31ms/step\n",
            "Epoch 1/2\n",
            "97/97 - 6s - 67ms/step - accuracy: 0.7132 - loss: 0.6749\n",
            "Epoch 2/2\n",
            "97/97 - 4s - 44ms/step - accuracy: 0.7453 - loss: 0.5986\n",
            "49/49 - 1s - 18ms/step\n",
            "Epoch 1/2\n",
            "97/97 - 7s - 71ms/step - accuracy: 0.7059 - loss: 0.6834\n",
            "Epoch 2/2\n",
            "97/97 - 5s - 49ms/step - accuracy: 0.7390 - loss: 0.6184\n",
            "49/49 - 1s - 18ms/step\n",
            "Epoch 1/2\n",
            "97/97 - 7s - 69ms/step - accuracy: 0.7190 - loss: 0.6717\n",
            "Epoch 2/2\n",
            "97/97 - 4s - 42ms/step - accuracy: 0.7474 - loss: 0.6070\n",
            "49/49 - 1s - 19ms/step\n",
            "Epoch 1/2\n",
            "97/97 - 7s - 73ms/step - accuracy: 0.7168 - loss: 0.6717\n",
            "Epoch 2/2\n",
            "97/97 - 4s - 44ms/step - accuracy: 0.7432 - loss: 0.6005\n",
            "49/49 - 1s - 20ms/step\n",
            "146/146 - 8s - 58ms/step - accuracy: 0.7118 - loss: 0.6785\n",
            "Best: 0.719681 using {'batch_size': 64, 'epochs': 1, 'optimizer': 'adam'}\n"
          ]
        }
      ]
    }
  ]
}